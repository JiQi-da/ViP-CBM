{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/disk/qij/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from pathlib import Path\n",
    "from pytorch_lightning import seed_everything\n",
    "from torchvision import transforms\n",
    "\n",
    "SELECTED_CONCEPTS = [\n",
    "    2,\n",
    "    4,\n",
    "    6,\n",
    "    7,\n",
    "    8,\n",
    "    9,\n",
    "    11,\n",
    "    12,\n",
    "    13,\n",
    "    14,\n",
    "    15,\n",
    "    16,\n",
    "    17,\n",
    "    18,\n",
    "    19,\n",
    "    20,\n",
    "    22,\n",
    "    23,\n",
    "    24,\n",
    "    25,\n",
    "    26,\n",
    "    27,\n",
    "    28,\n",
    "    29,\n",
    "    30,\n",
    "    32,\n",
    "    33,\n",
    "    39,\n",
    "]\n",
    "\n",
    "CONCEPT_SEMANTICS = [\n",
    "    '5_o_Clock_Shadow',\n",
    "    'Arched_Eyebrows',\n",
    "    'Attractive',\n",
    "    'Bags_Under_Eyes',\n",
    "    'Bald',\n",
    "    'Bangs',\n",
    "    'Big_Lips',\n",
    "    'Big_Nose',\n",
    "    'Black_Hair',\n",
    "    'Blond_Hair',\n",
    "    'Blurry',\n",
    "    'Brown_Hair',\n",
    "    'Bushy_Eyebrows',\n",
    "    'Chubby',\n",
    "    'Double_Chin',\n",
    "    'Eyeglasses',\n",
    "    'Goatee',\n",
    "    'Gray_Hair',\n",
    "    'Heavy_Makeup',\n",
    "    'High_Cheekbones',\n",
    "    'Male',\n",
    "    'Mouth_Slightly_Open',\n",
    "    'Mustache',\n",
    "    'Narrow_Eyes',\n",
    "    'No_Beard',\n",
    "    'Oval_Face',\n",
    "    'Pale_Skin',\n",
    "    'Pointy_Nose',\n",
    "    'Receding_Hairline',\n",
    "    'Rosy_Cheeks',\n",
    "    'Sideburns',\n",
    "    'Smiling',\n",
    "    'Straight_Hair',\n",
    "    'Wavy_Hair',\n",
    "    'Wearing_Earrings',\n",
    "    'Wearing_Hat',\n",
    "    'Wearing_Lipstick',\n",
    "    'Wearing_Necklace',\n",
    "    'Wearing_Necktie',\n",
    "    'Young',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate data\n",
    "`config`\n",
    "`seed`\n",
    "`output_data_var`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "root_dir = '../../celeba'\n",
    "concept_group_map = None\n",
    "seed_everything(42)\n",
    "use_binary_vector_class = True\n",
    "label_binary_width = 1\n",
    "width = 1\n",
    "\n",
    "def _binarize(concepts, selected, width):\n",
    "    result = []\n",
    "    binary_repr = []\n",
    "    concepts = concepts[selected]\n",
    "    for i in range(0, concepts.shape[-1], width):\n",
    "        binary_repr.append(\n",
    "            str(int(np.sum(concepts[i : i + width]) > 0))\n",
    "        )\n",
    "    return int(\"\".join(binary_repr), 2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import gdown\n",
    "celeba_train_data = torchvision.datasets.CelebA(\n",
    "            root=root_dir,\n",
    "            split='all',\n",
    "            download=True,\n",
    "            target_transform=lambda x: x[0].long() - 1,\n",
    "            # target_type=['attr'],\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 1,  ..., 0, 0, 1],\n",
      "        [0, 0, 0,  ..., 0, 0, 1],\n",
      "        [0, 0, 0,  ..., 0, 0, 1],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 1],\n",
      "        [0, 1, 1,  ..., 0, 0, 1],\n",
      "        [0, 1, 1,  ..., 0, 0, 1]])\n",
      "<bound method CelebA.extra_repr of Dataset CelebA\n",
      "    Number of datapoints: 202599\n",
      "    Root location: ../../celeba\n",
      "    Target type: ['attr']\n",
      "    Split: all\n",
      "    StandardTransform\n",
      "Target transform: <function <lambda> at 0x7ea838176a20>>\n",
      "(<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=178x218 at 0x7EA83816EE10>, tensor([0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
      "        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1]))\n"
     ]
    }
   ],
   "source": [
    "celeba_train_data.target_transform = lambda x: x#@x[0].long() - 1\n",
    "celeba_train_data.target_type = ['attr']\n",
    "print(celeba_train_data.attr)\n",
    "print(celeba_train_data.extra_repr)\n",
    "print(celeba_train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concept frequency is: [0.11113579 0.26698059 0.512505   0.20457159 0.02244335 0.15157528\n",
      " 0.24079586 0.23453225 0.23925093 0.14799185 0.05089857 0.20519351\n",
      " 0.14216753 0.05756692 0.04668829 0.06511878 0.06276438 0.04194986\n",
      " 0.38692195 0.45503186 0.41675428 0.48342786 0.04154512 0.11514864\n",
      " 0.83493996 0.28414257 0.0429469  0.27744461 0.07977828 0.06572096\n",
      " 0.05651064 0.48208037 0.20840182 0.31956722 0.18892492 0.04846026\n",
      " 0.4724357  0.12296704 0.07271507 0.77361685]\n",
      "[2, 21, 31, 36, 19, 20, 18, 33, 25, 27, 1, 6, 8, 7, 39, 32, 11, 3, 34, 24, 5, 9, 12, 37, 23, 0, 28, 38, 29, 15, 16, 13, 30, 10, 35, 14, 26, 17, 22, 4] \n",
      " [0.512505   0.48342786 0.48208037 0.4724357  0.45503186 0.41675428\n",
      " 0.38692195 0.31956722 0.28414257 0.27744461 0.26698059 0.24079586\n",
      " 0.23925093 0.23453225 0.77361685 0.20840182 0.20519351 0.20457159\n",
      " 0.18892492 0.83493996 0.15157528 0.14799185 0.14216753 0.12296704\n",
      " 0.11514864 0.11113579 0.07977828 0.07271507 0.06572096 0.06511878\n",
      " 0.06276438 0.05756692 0.05651064 0.05089857 0.04846026 0.04668829\n",
      " 0.0429469  0.04194986 0.04154512 0.02244335]\n",
      "[2, 19, 20, 21, 31, 36]\n"
     ]
    }
   ],
   "source": [
    "concept_freq = np.sum(\n",
    "    celeba_train_data.attr.cpu().detach().numpy(),\n",
    "    axis=0\n",
    ") / celeba_train_data.attr.shape[0]\n",
    "print(f\"Concept frequency is: {concept_freq}\")\n",
    "sorted_concepts = list(map(\n",
    "    lambda x: x[0],\n",
    "    sorted(enumerate(np.abs(concept_freq - 0.5)), key=lambda x: x[1]),\n",
    "))\n",
    "print(sorted_concepts, '\\n', concept_freq[sorted_concepts])\n",
    "num_concepts = 6\n",
    "concept_idxs = sorted_concepts[:num_concepts]\n",
    "concept_idxs = sorted(concept_idxs)\n",
    "print(concept_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18, 33]\n"
     ]
    }
   ],
   "source": [
    "num_hidden = 2\n",
    "hidden_concepts = sorted(\n",
    "                sorted_concepts[\n",
    "                    num_concepts:min(\n",
    "                        (num_concepts + num_hidden),\n",
    "                        len(sorted_concepts)\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "print(hidden_concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting concepts: [2, 19, 20, 21, 31, 36]\n",
      "\tAnd hidden concepts: [18, 33]\n",
      "Files already downloaded and verified\n",
      "torch.Size([202599, 40]) torch.Size([202599, 1])\n",
      "(tensor([[[ 1.0000,  1.0000,  1.0000,  ...,  0.9451,  0.9529,  0.9529],\n",
      "         [ 0.9922,  0.9922,  0.9922,  ...,  0.9451,  0.9529,  0.9451],\n",
      "         [ 0.9922,  0.9922,  0.9922,  ...,  0.9294,  0.9608,  0.9529],\n",
      "         ...,\n",
      "         [ 0.3020,  0.2627,  0.1137,  ...,  0.4196,  0.4510,  0.4510],\n",
      "         [ 0.3569,  0.2941,  0.2627,  ...,  0.3725,  0.3490,  0.3647],\n",
      "         [ 0.3882,  0.3647,  0.3804,  ...,  0.5608,  0.4824,  0.4196]],\n",
      "\n",
      "        [[ 0.8431,  0.8431,  0.8431,  ...,  0.8667,  0.8588,  0.8902],\n",
      "         [ 0.8431,  0.8431,  0.8431,  ...,  0.8824,  0.8667,  0.8980],\n",
      "         [ 0.8588,  0.8588,  0.8588,  ...,  0.8902,  0.8745,  0.8902],\n",
      "         ...,\n",
      "         [-0.2078, -0.2235, -0.3255,  ...,  0.0275,  0.0510,  0.0510],\n",
      "         [-0.1529, -0.2078, -0.2314,  ..., -0.0667, -0.0667, -0.0353],\n",
      "         [-0.1294, -0.1373, -0.1059,  ...,  0.1451,  0.0824,  0.0275]],\n",
      "\n",
      "        [[ 0.5529,  0.5529,  0.5529,  ...,  0.6863,  0.7490,  0.7961],\n",
      "         [ 0.5529,  0.5529,  0.5529,  ...,  0.7176,  0.7569,  0.8039],\n",
      "         [ 0.5529,  0.5529,  0.5529,  ...,  0.7333,  0.7569,  0.7882],\n",
      "         ...,\n",
      "         [-0.4824, -0.5137, -0.6549,  ..., -0.1922, -0.1608, -0.1608],\n",
      "         [-0.4510, -0.4980, -0.5059,  ..., -0.3255, -0.2706, -0.2392],\n",
      "         [-0.4667, -0.4745, -0.4196,  ..., -0.1451, -0.1843, -0.2471]]]), [tensor(222), tensor([1., 1., 0., 1., 1., 1.])])\n"
     ]
    }
   ],
   "source": [
    "image_size = 64\n",
    "print(f\"Selecting concepts: {concept_idxs}\")\n",
    "print(f\"\\tAnd hidden concepts: {hidden_concepts}\")\n",
    "celeba_train_data = torchvision.datasets.CelebA(\n",
    "    root=root_dir,\n",
    "    split='all',\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.CenterCrop(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.ConvertImageDtype(torch.float32),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ]),\n",
    "    target_transform=lambda x: [\n",
    "        torch.tensor(\n",
    "            _binarize(\n",
    "                x[1].cpu().detach().numpy(),\n",
    "                selected=(concept_idxs + hidden_concepts),\n",
    "                width=width,\n",
    "            ),\n",
    "            dtype=torch.long,\n",
    "        ),\n",
    "        x[1][concept_idxs].float(),\n",
    "    ],\n",
    "    target_type=['identity', 'attr'],\n",
    ")\n",
    "print(celeba_train_data.attr.shape, celeba_train_data.identity.shape)\n",
    "print(celeba_train_data[0]) # (image, [class, concept])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  44  45  46  48  49  50  52  53  54  56  57  59\n",
      "  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77\n",
      "  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95\n",
      "  96  97  99 100 102 104 105 106 108 109 110 111 112 113 114 116 120 121\n",
      " 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139\n",
      " 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157\n",
      " 158 159 160 161 162 163 164 165 166 167 168 169 170 172 173 174 175 176\n",
      " 177 178 180 181 182 183 184 185 186 188 189 190 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 228 230 231 232 233 234 236\n",
      " 237 238 239 240 241 244 248 249 250 251 252 253 254 255] [ 4552  1292   288    85  1581  1083  1811  1281   237    60     9     4\n",
      "    69    57    94    70  1916   411    85    24   483   394   863   707\n",
      "   730   198    21    12   206   178   177   167 22742  3666    34     1\n",
      "    38    26    18     7  2111   361     1     1     2  7041   936     5\n",
      "    11     8    10  4493   668     1     6     6     4     1   517   166\n",
      "    32    17   251   240   479   482   451   192    25    13   415   366\n",
      "   642   770   461    80    17     4   112    75   207   212  2876   832\n",
      "    82    33  1580  1268  2770  3099  2133   268     1     4     1  2664\n",
      "   382     1     6     3     3     1  1370   103     2     1 10581  1089\n",
      "     1     2    28     6     6     1  2015   793   400   152  2296  1684\n",
      "  7727  7501   119    45    18    10   154   105   690   573   306   139\n",
      "    69    34   470   383  2044  2581   265    95    28    17   274   177\n",
      "   854   790  8474  1974    19     5    75    22    32     6  1344   363\n",
      "     3     4     5     1     2   825   198     2    11     2     1     1\n",
      "  2391   480     1     9     3     6   152    65    35    15   265   238\n",
      "  1808  2009   263   120    38    15   576   395  3962  4200    32    14\n",
      "    10     8    44    42   313   442  1350   454   124    51  2021  1329\n",
      " 12552 14504   373    98     1     6     2     1  1011   260     4    13\n",
      "     4    10     1    65    15     2  4452   885     8     4    68    15\n",
      "    12    10]\n"
     ]
    }
   ],
   "source": [
    "label_remap = {}\n",
    "vals, counts = np.unique(\n",
    "    list(map(\n",
    "        lambda x: _binarize(\n",
    "            x.cpu().detach().numpy(),\n",
    "            selected=(concept_idxs + hidden_concepts),\n",
    "            width=width,\n",
    "        ),\n",
    "        celeba_train_data.attr\n",
    "    )),\n",
    "    return_counts=True,\n",
    ")\n",
    "\n",
    "print(vals, counts)\n",
    "for i, label in enumerate(vals):\n",
    "    label_remap[label] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 17, 18: 18, 19: 19, 20: 20, 21: 21, 22: 22, 23: 23, 24: 24, 25: 25, 26: 26, 27: 27, 28: 28, 29: 29, 30: 30, 31: 31, 32: 32, 33: 33, 34: 34, 35: 35, 36: 36, 37: 37, 38: 38, 39: 39, 40: 40, 41: 41, 44: 42, 45: 43, 46: 44, 48: 45, 49: 46, 50: 47, 52: 48, 53: 49, 54: 50, 56: 51, 57: 52, 59: 53, 60: 54, 61: 55, 62: 56, 63: 57, 64: 58, 65: 59, 66: 60, 67: 61, 68: 62, 69: 63, 70: 64, 71: 65, 72: 66, 73: 67, 74: 68, 75: 69, 76: 70, 77: 71, 78: 72, 79: 73, 80: 74, 81: 75, 82: 76, 83: 77, 84: 78, 85: 79, 86: 80, 87: 81, 88: 82, 89: 83, 90: 84, 91: 85, 92: 86, 93: 87, 94: 88, 95: 89, 96: 90, 97: 91, 99: 92, 100: 93, 102: 94, 104: 95, 105: 96, 106: 97, 108: 98, 109: 99, 110: 100, 111: 101, 112: 102, 113: 103, 114: 104, 116: 105, 120: 106, 121: 107, 122: 108, 123: 109, 124: 110, 125: 111, 126: 112, 127: 113, 128: 114, 129: 115, 130: 116, 131: 117, 132: 118, 133: 119, 134: 120, 135: 121, 136: 122, 137: 123, 138: 124, 139: 125, 140: 126, 141: 127, 142: 128, 143: 129, 144: 130, 145: 131, 146: 132, 147: 133, 148: 134, 149: 135, 150: 136, 151: 137, 152: 138, 153: 139, 154: 140, 155: 141, 156: 142, 157: 143, 158: 144, 159: 145, 160: 146, 161: 147, 162: 148, 163: 149, 164: 150, 165: 151, 166: 152, 167: 153, 168: 154, 169: 155, 170: 156, 172: 157, 173: 158, 174: 159, 175: 160, 176: 161, 177: 162, 178: 163, 180: 164, 181: 165, 182: 166, 183: 167, 184: 168, 185: 169, 186: 170, 188: 171, 189: 172, 190: 173, 192: 174, 193: 175, 194: 176, 195: 177, 196: 178, 197: 179, 198: 180, 199: 181, 200: 182, 201: 183, 202: 184, 203: 185, 204: 186, 205: 187, 206: 188, 207: 189, 208: 190, 209: 191, 210: 192, 211: 193, 212: 194, 213: 195, 214: 196, 215: 197, 216: 198, 217: 199, 218: 200, 219: 201, 220: 202, 221: 203, 222: 204, 223: 205, 224: 206, 225: 207, 226: 208, 228: 209, 230: 210, 231: 211, 232: 212, 233: 213, 234: 214, 236: 215, 237: 216, 238: 217, 239: 218, 240: 219, 241: 220, 244: 221, 248: 222, 249: 223, 250: 224, 251: 225, 252: 226, 253: 227, 254: 228, 255: 229}\n"
     ]
    }
   ],
   "source": [
    "print(label_remap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "230\n",
      "torch.Size([202599, 40]) torch.Size([202599, 1])\n",
      "(tensor([[[ 1.0000,  1.0000,  1.0000,  ...,  0.9451,  0.9529,  0.9529],\n",
      "         [ 0.9922,  0.9922,  0.9922,  ...,  0.9451,  0.9529,  0.9451],\n",
      "         [ 0.9922,  0.9922,  0.9922,  ...,  0.9294,  0.9608,  0.9529],\n",
      "         ...,\n",
      "         [ 0.3020,  0.2627,  0.1137,  ...,  0.4196,  0.4510,  0.4510],\n",
      "         [ 0.3569,  0.2941,  0.2627,  ...,  0.3725,  0.3490,  0.3647],\n",
      "         [ 0.3882,  0.3647,  0.3804,  ...,  0.5608,  0.4824,  0.4196]],\n",
      "\n",
      "        [[ 0.8431,  0.8431,  0.8431,  ...,  0.8667,  0.8588,  0.8902],\n",
      "         [ 0.8431,  0.8431,  0.8431,  ...,  0.8824,  0.8667,  0.8980],\n",
      "         [ 0.8588,  0.8588,  0.8588,  ...,  0.8902,  0.8745,  0.8902],\n",
      "         ...,\n",
      "         [-0.2078, -0.2235, -0.3255,  ...,  0.0275,  0.0510,  0.0510],\n",
      "         [-0.1529, -0.2078, -0.2314,  ..., -0.0667, -0.0667, -0.0353],\n",
      "         [-0.1294, -0.1373, -0.1059,  ...,  0.1451,  0.0824,  0.0275]],\n",
      "\n",
      "        [[ 0.5529,  0.5529,  0.5529,  ...,  0.6863,  0.7490,  0.7961],\n",
      "         [ 0.5529,  0.5529,  0.5529,  ...,  0.7176,  0.7569,  0.8039],\n",
      "         [ 0.5529,  0.5529,  0.5529,  ...,  0.7333,  0.7569,  0.7882],\n",
      "         ...,\n",
      "         [-0.4824, -0.5137, -0.6549,  ..., -0.1922, -0.1608, -0.1608],\n",
      "         [-0.4510, -0.4980, -0.5059,  ..., -0.3255, -0.2706, -0.2392],\n",
      "         [-0.4667, -0.4745, -0.4196,  ..., -0.1451, -0.1843, -0.2471]]]), [tensor(204), tensor([1., 1., 0., 1., 1., 1.])])\n"
     ]
    }
   ],
   "source": [
    "celeba_train_data = torchvision.datasets.CelebA(\n",
    "    root=root_dir,\n",
    "    split='all',\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.CenterCrop(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.ConvertImageDtype(torch.float32),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ]),\n",
    "    target_transform=lambda x: [\n",
    "        torch.tensor(\n",
    "            label_remap[_binarize(\n",
    "                x[1].cpu().detach().numpy(),\n",
    "                selected=(concept_idxs + hidden_concepts),\n",
    "                width=width,\n",
    "            )],\n",
    "            dtype=torch.long,\n",
    "        ),\n",
    "        x[1][concept_idxs].float(),\n",
    "    ],\n",
    "    target_type=['identity', 'attr'],\n",
    ")\n",
    "num_classes = len(label_remap)\n",
    "print(num_classes)\n",
    "print(celeba_train_data.attr.shape, celeba_train_data.identity.shape)\n",
    "print(celeba_train_data[0]) # (image, [class, concept])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16883\n"
     ]
    }
   ],
   "source": [
    "label_dataset_subsample = 12\n",
    "# subsample the dataset\n",
    "factor = label_dataset_subsample\n",
    "if factor != 1:\n",
    "    train_idxs = np.random.choice(\n",
    "        np.arange(0, len(celeba_train_data)),\n",
    "        replace=False,\n",
    "        size=len(celeba_train_data)//factor,\n",
    "    )\n",
    "    logging.debug(f\"Subsampling to {len(train_idxs)} elements.\")\n",
    "    celeba_train_data = torch.utils.data.Subset(\n",
    "        celeba_train_data,\n",
    "        train_idxs,\n",
    "    )\n",
    "    print(celeba_train_data.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 4\n",
    "total_samples = len(celeba_train_data)\n",
    "train_samples = int(0.7 * total_samples)\n",
    "test_samples = int(0.2 * total_samples)\n",
    "val_samples = total_samples - test_samples - train_samples\n",
    "logging.debug(\n",
    "    f\"Data split is: {total_samples} = {train_samples} (train) + \"\n",
    "    f\"{test_samples} (test) + {val_samples} (validation)\"\n",
    ")\n",
    "celeba_train_data, celeba_test_data, celeba_val_data = \\\n",
    "    torch.utils.data.random_split(\n",
    "        celeba_train_data,\n",
    "        [train_samples, test_samples, val_samples],\n",
    "    )\n",
    "train_dl = torch.utils.data.DataLoader(\n",
    "    celeba_train_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "test_dl = torch.utils.data.DataLoader(\n",
    "    celeba_test_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "val_dl = torch.utils.data.DataLoader(\n",
    "    celeba_val_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当你使用 `torchvision.datasets.CelebA` 下载 CelebA 数据集时，数据集的文件格式和结构如下所示：\n",
    "\n",
    "### 下载后的文件结构：\n",
    "下载后，CelebA 数据集将被存储在指定的 `root` 目录下，通常包括以下内容：\n",
    "\n",
    "```\n",
    "root/\n",
    "    CelebA/\n",
    "        img_align_celeba/        # 存放图片的文件夹\n",
    "        list_attr_celeba.txt      # 属性标签文件\n",
    "        list_eval_partition.txt   # 数据集划分文件\n",
    "        list_identity_celeba.txt  # 人物身份标签文件\n",
    "        list_landmarks_celeba.txt # 关键点标签文件\n",
    "```\n",
    "\n",
    "#### 各个文件说明：\n",
    "1. **`img_align_celeba/`**  \n",
    "   这个文件夹包含了所有的 CelebA 图片。图片的命名格式通常是以 `000001.jpg`, `000002.jpg` 这样的数字作为文件名。每张图片的尺寸为 `178x218`，它们通常会经过对齐和裁剪。\n",
    "\n",
    "2. **`list_attr_celeba.txt`**  \n",
    "   该文件包含了每张图片的属性标签，每行对应一张图片的多个二进制属性（例如：是否微笑、是否戴眼镜、是否穿着耳环等）。文件内容通常是这样的：\n",
    "   ```\n",
    "   000001.jpg 1 -1 1 -1 1 1 1 -1 1 0 ...  # 对应于图片000001.jpg的多个属性\n",
    "   000002.jpg -1 1 -1 1 -1 1 1 1 -1 1 ...  # 对应于图片000002.jpg的多个属性\n",
    "   ```\n",
    "\n",
    "3. **`list_eval_partition.txt`**  \n",
    "   该文件包含了数据集的划分信息，用于训练、验证和测试。它指示每张图片属于哪个数据集（训练集、验证集或测试集）。文件内容通常是这样的：\n",
    "   ```\n",
    "   000001.jpg 0\n",
    "   000002.jpg 1\n",
    "   000003.jpg 2\n",
    "   ...\n",
    "   ```\n",
    "   其中，`0` 表示训练集，`1` 表示验证集，`2` 表示测试集。\n",
    "\n",
    "4. **`list_identity_celeba.txt`**  \n",
    "   该文件包含每张图片的人物身份标签。每个标签是一个整数，表示该图片属于哪个身份（人物）。内容类似：\n",
    "   ```\n",
    "   000001.jpg 1\n",
    "   000002.jpg 2\n",
    "   000003.jpg 1\n",
    "   ...\n",
    "   ```\n",
    "   其中，`1`, `2` 等是人物的ID编号。\n",
    "\n",
    "5. **`list_landmarks_celeba.txt`**  \n",
    "   该文件包含每张图片的五个关键点的坐标（如眼睛、鼻子、嘴巴的位置）。内容可能类似于：\n",
    "   ```\n",
    "   000001.jpg x1 y1 x2 y2 x3 y3 x4 y4 x5 y5\n",
    "   000002.jpg x1 y1 x2 y2 x3 y3 x4 y4 x5 y5\n",
    "   ...\n",
    "   ```\n",
    "   每行表示一张图片的五个面部关键点的坐标。\n",
    "\n",
    "### 读取和使用：\n",
    "- **图片**：你可以通过 `torchvision.datasets.CelebA` 直接加载图片（例如通过 `transform` 转换为 Tensor）。\n",
    "- **属性标签**：你可以通过 `attr` 参数来获取每张图片的属性标签，例如是否微笑、是否戴眼镜等。\n",
    "- **数据集划分**：你可以使用 `split` 参数来加载不同的数据集部分（训练集、验证集、测试集）。\n",
    "- **关键点信息**：如果需要，可以使用 `landmarks` 参数来加载每张图片的五个关键点坐标。\n",
    "\n",
    "例如：\n",
    "\n",
    "```python\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "data_dir = '/path/to/CelebA'\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# 加载训练集\n",
    "celeba_dataset = datasets.CelebA(root=data_dir, split='train', transform=transform, download=False)\n",
    "\n",
    "# 查看第一个样本\n",
    "image, label = celeba_dataset[0]\n",
    "print(image.shape)  # 图片大小 (3, 128, 128)\n",
    "print(label)        # 属性标签\n",
    "```\n",
    "\n",
    "### 小结：\n",
    "下载后的 CelebA 数据集包含了图片、属性标签、身份标签、数据集划分信息等。你可以根据需要，选择性地加载和处理这些文件。如果你需要处理属性标签、身份标签或关键点信息，可以从相应的文本文件中读取。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
